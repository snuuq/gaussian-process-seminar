@book{Garnett2023-gc,
  title     = {Bayesian optimization},
  author    = {Garnett, Roman},
  publisher = {Cambridge University Press},
  location  = {Cambridge, England},
  date      = {2023-02-09},
  year      = {2023},
  pagetotal = {358},
  abstract  = {Bayesian optimization is a methodology for optimizing expensive
               objective functions that has proven success in the sciences,
               engineering, and beyond. This timely text provides a
               self-contained and comprehensive introduction to the subject,
               starting from scratch and carefully developing all the key ideas
               along the way. This bottom-up approach illuminates unifying
               themes in the design of Bayesian optimization algorithms and
               builds a solid theoretical foundation for approaching novel
               situations. The core of the book is divided into three main
               parts, covering theoretical and practical aspects of Gaussian
               process modeling, the Bayesian approach to sequential decision
               making, and the realization and computation of practical and
               effective optimization policies. Following this foundational
               material, the book provides an overview of theoretical
               convergence results, a survey of notable extensions, a
               comprehensive history of Bayesian optimization, and an extensive
               annotated bibliography of applications.},
  language  = {en}
}, 
@book{Durrett2019-gs,
  title     = {Probability: Theory and examples},
  author    = {Durrett, Rick},
  publisher = {Cambridge University Press},
  location  = {Cambridge, England},
  date      = {2019-04-18},
  year      = {2019},
  pagetotal = {430},
  series    = {Cambridge Series in Statistical and Probabilistic Mathematics},
  language  = {en}
},
@book{Le-Gall2016-ik,
  title     = {Brownian motion, martingales, and stochastic calculus},
  author    = {Le Gall, Jean-Francois},
  publisher = {Springer International Publishing},
  location  = {Cham, Switzerland},
  edition   = {2016},
  date      = {2016-04-29},
  year      = {2016},
  pagetotal = {273},
  series    = {Graduate texts in mathematics},
  language  = {en}
},
@book{Oksendal2003-nf,
  title     = {Stochastic differential equations: An introduction with
               applications},
  author    = {Oksendal, Bernt},
  publisher = {Springer},
  location  = {Berlin, Germany},
  date      = {2003-07-15},
  year      = {2003},
  pagetotal = {379},
  series    = {Universitext},
  language  = {en}
},
@article{Alvarez2011-vl,
  title        = {Kernels for vector-valued functions: A review},
  author       = {Alvarez, Mauricio A and Rosasco, Lorenzo and Lawrence, Neil D},
  journaltitle = {arXiv [stat.ML]},
  date         = {2011-06-30},
  year         = {2011},
  eprinttype   = {arXiv},
  eprintclass  = {stat.ML},
  abstract     = {Kernel methods are among the most popular techniques in
                  machine learning. From a frequentist/discriminative
                  perspective they play a central role in regularization theory
                  as they provide a natural choice for the hypotheses space and
                  the regularization functional through the notion of
                  reproducing kernel Hilbert spaces. From a Bayesian/generative
                  perspective they are the key in the context of Gaussian
                  processes, where the kernel function is also known as the
                  covariance function. Traditionally, kernel methods have been
                  used in supervised learning problem with scalar outputs and
                  indeed there has been a considerable amount of work devoted to
                  designing and learning kernels. More recently there has been
                  an increasing interest in methods that deal with multiple
                  outputs, motivated partly by frameworks like multitask
                  learning. In this paper, we review different methods to design
                  or learn valid kernel functions for multiple outputs, paying
                  particular attention to the connection between probabilistic
                  and functional methods.}
}

@inbook{Sarkka2011-vy,
  title     = {Linear operators and stochastic partial differential equations in
               Gaussian process regression},
  author    = {Särkkä, Simo},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  location  = {Berlin, Heidelberg},
  pages     = {151--158},
  date      = {2011},
  year      = {2011},
  abstract  = {In this paper we shall discuss an extension to Gaussian process
               (GP) regression models, where the measurements are modeled as
               linear functionals of the underlying GP and the estimation
               objective is a general linear operator of the process. We shall
               show how this framework can be used for modeling physical
               processes involved in measurement of the GP and for encoding
               physical prior information into regression models in form of
               stochastic partial differential equations (SPDE). We shall also
               illustrate the practical applicability of the theory in a
               simulated application.},
  series    = {Lecture Notes in Computer Science},
  language  = {en}
}

